{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39a1d8eb",
   "metadata": {},
   "source": [
    "# üß† Enhanced Knowledge Graph System - Complete Setup & Testing Guide\n",
    "\n",
    "## üéØ Overview\n",
    "This notebook provides a comprehensive guide to set up and test the **Enhanced Chapter-Based Knowledge Graph System** with:\n",
    "\n",
    "- **Dynamic Concept Extraction**: 10-30 concepts per chapter (NOT hardcoded)\n",
    "- **5 Intelligent Extraction Methods**: Explicit objectives, key terms, technical concepts, section concepts, question concepts\n",
    "- **GPU Acceleration**: CUDA optimization throughout (CPU fallback available)\n",
    "- **Concept Clustering**: K-means clustering for better organization\n",
    "- **Neo4j Knowledge Graph**: Complex relations and prerequisites\n",
    "- **Advanced Visualizations**: 3D concept clusters, prerequisite networks, analytics dashboards\n",
    "- **Separate Database**: `gpu_chapter_knowledge_v1` preserving your original system\n",
    "\n",
    "## üö® Issues We'll Fix\n",
    "Based on your setup output, we need to address:\n",
    "1. ‚ùå `torch-audio` dependency issue (incompatible with Python 3.12)\n",
    "2. ‚ùå spaCy installation failure \n",
    "3. ‚ùå Neo4j service not running\n",
    "4. ‚ö†Ô∏è Ollama qwen3:4b model missing\n",
    "5. ‚ö†Ô∏è No GPU available (will use CPU with optimization)\n",
    "\n",
    "Let's fix these step by step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22cbf47",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check System Prerequisites\n",
    "\n",
    "First, let's check your system compatibility and identify what needs to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa486a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üß† ENHANCED KNOWLEDGE GRAPH SYSTEM - PREREQUISITES CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check Python version\n",
    "print(f\"üêç Python Version: {sys.version}\")\n",
    "print(f\"   Platform: {platform.platform()}\")\n",
    "print(f\"   Architecture: {platform.architecture()[0]}\")\n",
    "\n",
    "# Check if we're in the right directory\n",
    "current_dir = Path.cwd()\n",
    "print(f\"üìÅ Current Directory: {current_dir}\")\n",
    "\n",
    "# Check for key files\n",
    "key_files = [\n",
    "    \"COMPLETE_ENHANCED_RUNNER.py\",\n",
    "    \"enhanced_requirements.txt\", \n",
    "    \"elasticsearch_enhanced_relations.py\",\n",
    "    \"docker-compose-elasticsearch.yml\"\n",
    "]\n",
    "\n",
    "print(\"\\nüìã Checking Key Files:\")\n",
    "for file in key_files:\n",
    "    file_path = current_dir / file\n",
    "    status = \"‚úÖ Found\" if file_path.exists() else \"‚ùå Missing\"\n",
    "    print(f\"   {status}: {file}\")\n",
    "\n",
    "# Check data directory\n",
    "data_dir = current_dir / \"data_large\"\n",
    "if data_dir.exists():\n",
    "    file_count = len(list(data_dir.glob(\"*\")))\n",
    "    print(f\"‚úÖ Data directory exists with {file_count} files\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data directory 'data_large' not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa97a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced GPU availability check with detailed CUDA information\n",
    "print(\"üöÄ COMPREHENSIVE GPU & CUDA AVAILABILITY CHECK\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch installed: {torch.__version__}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        gpu_count = torch.cuda.device_count()\n",
    "        \n",
    "        print(f\"üéÆ GPU ACCELERATION: AVAILABLE\")\n",
    "        print(f\"   GPU Count: {gpu_count}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "        \n",
    "        # Detailed GPU information for each device\n",
    "        for i in range(gpu_count):\n",
    "            gpu_props = torch.cuda.get_device_properties(i)\n",
    "            gpu_name = gpu_props.name\n",
    "            total_memory = gpu_props.total_memory / 1024**3\n",
    "            \n",
    "            print(f\"\\n   \udfaf GPU {i}: {gpu_name}\")\n",
    "            print(f\"      Memory: {total_memory:.1f} GB\")\n",
    "            print(f\"      Compute Capability: {gpu_props.major}.{gpu_props.minor}\")\n",
    "            print(f\"      Multi-processors: {gpu_props.multi_processor_count}\")\n",
    "            \n",
    "            # Check current GPU utilization\n",
    "            try:\n",
    "                torch.cuda.set_device(i)\n",
    "                torch.cuda.empty_cache()\n",
    "                allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "                cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "                print(f\"      Current Usage: {allocated:.2f} GB allocated, {cached:.2f} GB cached\")\n",
    "            except Exception as e:\n",
    "                print(f\"      Usage check failed: {e}\")\n",
    "        \n",
    "        # Test GPU performance\n",
    "        print(f\"\\nüî• GPU PERFORMANCE TEST:\")\n",
    "        try:\n",
    "            # Create test tensors and measure performance\n",
    "            import time\n",
    "            test_size = 1000\n",
    "            \n",
    "            # CPU test  \n",
    "            start_time = time.time()\n",
    "            cpu_tensor = torch.randn(test_size, test_size)\n",
    "            cpu_result = torch.mm(cpu_tensor, cpu_tensor)\n",
    "            cpu_time = time.time() - start_time\n",
    "            \n",
    "            # GPU test\n",
    "            start_time = time.time()\n",
    "            gpu_tensor = torch.randn(test_size, test_size).cuda()\n",
    "            gpu_result = torch.mm(gpu_tensor, gpu_tensor)\n",
    "            torch.cuda.synchronize()  # Wait for GPU operation to complete\n",
    "            gpu_time = time.time() - start_time\n",
    "            \n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"   CPU Time: {cpu_time:.4f} seconds\")\n",
    "            print(f\"   GPU Time: {gpu_time:.4f} seconds\") \n",
    "            print(f\"   üöÄ GPU Speedup: {speedup:.1f}x faster\")\n",
    "            \n",
    "            if speedup > 5:\n",
    "                print(f\"   ‚úÖ Excellent GPU performance!\")\n",
    "            elif speedup > 2:\n",
    "                print(f\"   ‚úÖ Good GPU performance\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è Limited GPU benefit (check GPU utilization)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå GPU performance test failed: {e}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è NO GPU AVAILABLE - USING CPU MODE\")\n",
    "        print(\"   Reasons might include:\")\n",
    "        print(\"   ‚Ä¢ No NVIDIA GPU installed\")\n",
    "        print(\"   ‚Ä¢ CUDA drivers not installed\")\n",
    "        print(\"   ‚Ä¢ PyTorch CPU-only version installed\")\n",
    "        print(\"   ‚Ä¢ GPU not compatible with CUDA\")\n",
    "        print(\"\\n   üí° OPTIMIZATION FOR CPU:\")\n",
    "        print(\"   ‚Ä¢ System will use multi-threading\")\n",
    "        print(\"   ‚Ä¢ Batch sizes will be automatically reduced\")\n",
    "        print(\"   ‚Ä¢ Processing will be 2-5x slower than GPU\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå PyTorch not installed\")\n",
    "    print(\"   Install with: pip install torch --index-url https://download.pytorch.org/whl/cu121\")\n",
    "\n",
    "# Check additional GPU libraries\n",
    "print(f\"\\n\udd0d GPU LIBRARY ECOSYSTEM:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check CuPy (GPU-accelerated NumPy)\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(f\"‚úÖ CuPy (GPU NumPy): {cp.__version__}\")\n",
    "    \n",
    "    # Test CuPy performance\n",
    "    try:\n",
    "        import numpy as np\n",
    "        test_array = np.random.rand(1000, 1000)\n",
    "        \n",
    "        # NumPy CPU\n",
    "        start_time = time.time()\n",
    "        np_result = np.dot(test_array, test_array)\n",
    "        numpy_time = time.time() - start_time\n",
    "        \n",
    "        # CuPy GPU  \n",
    "        gpu_array = cp.asarray(test_array)\n",
    "        start_time = time.time()\n",
    "        cp_result = cp.dot(gpu_array, gpu_array)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        cupy_time = time.time() - start_time\n",
    "        \n",
    "        cupy_speedup = numpy_time / cupy_time\n",
    "        print(f\"   üöÄ CuPy vs NumPy speedup: {cupy_speedup:.1f}x\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è CuPy performance test failed: {e}\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå CuPy not available (install with: pip install cupy-cuda12x)\")\n",
    "\n",
    "# Check GPU monitoring tools\n",
    "try:\n",
    "    import nvidia_ml_py3 as nvml\n",
    "    nvml.nvmlInit()\n",
    "    driver_version = nvml.nvmlSystemGetDriverVersion()\n",
    "    print(f\"‚úÖ NVIDIA-ML: Driver {driver_version}\")\n",
    "    \n",
    "    # Get detailed GPU info\n",
    "    device_count = nvml.nvmlDeviceGetCount()\n",
    "    for i in range(device_count):\n",
    "        handle = nvml.nvmlDeviceGetHandleByIndex(i)\n",
    "        name = nvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "        \n",
    "        # Temperature\n",
    "        try:\n",
    "            temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)\n",
    "            print(f\"   üå°Ô∏è {name}: {temp}¬∞C\")\n",
    "        except:\n",
    "            print(f\"   üéÆ {name}: Temperature unavailable\")\n",
    "            \n",
    "        # Power usage\n",
    "        try:\n",
    "            power = nvml.nvmlDeviceGetPowerUsage(handle) / 1000  # Convert to watts\n",
    "            print(f\"   ‚ö° Power usage: {power:.1f}W\")\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"‚ùå NVIDIA-ML not available (install with: pip install nvidia-ml-py3)\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è NVIDIA-ML error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ GPU OPTIMIZATION RECOMMENDATIONS:\")\n",
    "\n",
    "if torch.cuda.is_available() if 'torch' in locals() else False:\n",
    "    print(\"‚úÖ Your system is GPU-ready for maximum performance!\")\n",
    "    print(\"   ‚Ä¢ Use large batch sizes (64-128)\")\n",
    "    print(\"   ‚Ä¢ Enable mixed precision training\") \n",
    "    print(\"   ‚Ä¢ Use GPU-accelerated data loading\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è System will run in CPU mode:\")\n",
    "    print(\"   ‚Ä¢ Use smaller batch sizes (8-16)\")\n",
    "    print(\"   ‚Ä¢ Enable CPU multi-threading\")\n",
    "    print(\"   ‚Ä¢ Consider cloud GPU instances for better performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699c11e",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Python Dependencies (Fixed Version)\n",
    "\n",
    "Let's fix the dependency issues by creating a corrected requirements file and installing packages step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e8a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GPU-optimized requirements file with CUDA versions where available\n",
    "gpu_optimized_requirements = \"\"\"\n",
    "# GPU-Accelerated ML and Deep Learning (CUDA versions)\n",
    "torch>=2.0.0+cu121\n",
    "torchvision>=0.15.0+cu121\n",
    "torchaudio>=2.0.0+cu121\n",
    "transformers>=4.30.0\n",
    "sentence-transformers>=2.2.0\n",
    "scikit-learn>=1.3.0\n",
    "\n",
    "# GPU-accelerated data processing\n",
    "cupy-cuda12x>=12.0.0\n",
    "cudf>=23.10.0\n",
    "cuml>=23.10.0\n",
    "cugraph>=23.10.0\n",
    "\n",
    "# GPU-enhanced NLP Libraries\n",
    "spacy[cuda121]>=3.6.0\n",
    "nltk>=3.8.0\n",
    "textstat>=0.7.0\n",
    "\n",
    "# GPU-optimized numerical computing\n",
    "numpy>=1.24.0\n",
    "pandas>=2.0.0\n",
    "scipy>=1.11.0\n",
    "\n",
    "# Database and Storage with GPU support\n",
    "elasticsearch>=8.0.0\n",
    "neo4j>=5.0.0\n",
    "\n",
    "# GPU-accelerated visualization\n",
    "plotly>=5.15.0\n",
    "matplotlib>=3.7.0\n",
    "seaborn>=0.12.0\n",
    "networkx>=3.1.0\n",
    "\n",
    "# GPU monitoring and optimization\n",
    "nvidia-ml-py3>=7.352.0\n",
    "gpustat>=1.1.1\n",
    "psutil>=5.9.0\n",
    "\n",
    "# Memory optimization for GPU\n",
    "memory-profiler>=0.61.0\n",
    "py3nvml>=0.2.7\n",
    "\n",
    "# System utilities\n",
    "requests>=2.31.0\n",
    "tqdm>=4.65.0\n",
    "python-dotenv>=1.0.0\n",
    "colorama>=0.4.6\n",
    "\n",
    "# Text processing utilities (GPU-compatible versions)\n",
    "beautifulsoup4>=4.12.0\n",
    "markdown>=3.4.0\n",
    "python-docx>=0.8.11\n",
    "PyPDF2>=3.0.1\n",
    "pyyaml>=6.0.1\n",
    "\n",
    "# Development tools\n",
    "jupyter>=1.0.0\n",
    "notebook>=6.5.0\n",
    "pytest>=7.4.0\n",
    "black>=23.0.0\n",
    "\n",
    "# Web framework (optional)\n",
    "fastapi>=0.100.0\n",
    "uvicorn>=0.23.0\n",
    "\n",
    "# CUDA-specific installations (will be handled separately)\n",
    "# faiss-gpu>=1.7.4\n",
    "# rapids-blazingsql>=21.12.0\n",
    "\"\"\".strip()\n",
    "\n",
    "# Write the GPU-optimized requirements\n",
    "with open(\"enhanced_requirements_gpu.txt\", \"w\") as f:\n",
    "    f.write(gpu_optimized_requirements)\n",
    "\n",
    "print(\"üöÄ Created enhanced_requirements_gpu.txt (GPU-OPTIMIZED)\")\n",
    "print(\"‚ö° Prioritizing CUDA versions for maximum GPU acceleration\")\n",
    "print(\"üéÆ Includes GPU monitoring and optimization tools\")\n",
    "\n",
    "# Display the GPU requirements\n",
    "print(\"\\nüìã GPU-Optimized Requirements:\")\n",
    "print(\"-\" * 40)\n",
    "for line in gpu_optimized_requirements.split('\\n'):\n",
    "    if line.strip() and not line.startswith('#') and 'cuda' in line.lower():\n",
    "        print(f\"   üéÆ {line.strip()}\")\n",
    "    elif line.strip() and not line.startswith('#') and ('gpu' in line.lower() or 'cu' in line.lower()):\n",
    "        print(f\"   ‚ö° {line.strip()}\")\n",
    "    elif line.strip() and not line.startswith('#'):\n",
    "        print(f\"   üì¶ {line.strip()}\")\n",
    "    elif line.startswith('#'):\n",
    "        print(f\"\\n{line}\")\n",
    "\n",
    "print(\"\\nüî• KEY GPU OPTIMIZATIONS:\")\n",
    "print(\"   ‚Ä¢ PyTorch with CUDA 12.1 support\")\n",
    "print(\"   ‚Ä¢ CuPy for GPU-accelerated NumPy operations\")\n",
    "print(\"   ‚Ä¢ cuDF/cuML for GPU-accelerated pandas/sklearn\")\n",
    "print(\"   ‚Ä¢ spaCy with CUDA support\")\n",
    "print(\"   ‚Ä¢ GPU monitoring with nvidia-ml-py3\")\n",
    "print(\"   ‚Ä¢ Memory optimization tools\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è NOTE: Some packages require NVIDIA GPU with CUDA 12.1+\")\n",
    "print(\"   System will gracefully fallback to CPU versions if GPU unavailable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"\ude80 INSTALLING GPU-OPTIMIZED PYTHON DEPENDENCIES\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def install_gpu_packages():\n",
    "    \"\"\"Install GPU packages with intelligent fallback to CPU versions\"\"\"\n",
    "    \n",
    "    # Step 1: Install PyTorch with CUDA support\n",
    "    print(\"1Ô∏è‚É£ Installing PyTorch with CUDA 12.1 support...\")\n",
    "    try:\n",
    "        torch_cmd = [\n",
    "            sys.executable, '-m', 'pip', 'install', \n",
    "            'torch>=2.0.0', 'torchvision>=0.15.0', 'torchaudio>=2.0.0',\n",
    "            '--index-url', 'https://download.pytorch.org/whl/cu121'\n",
    "        ]\n",
    "        result = subprocess.run(torch_cmd, capture_output=True, text=True, timeout=300)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úÖ PyTorch with CUDA installed successfully!\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è CUDA PyTorch failed, installing CPU version...\")\n",
    "            cpu_cmd = [sys.executable, '-m', 'pip', 'install', 'torch', 'torchvision', 'torchaudio']\n",
    "            subprocess.run(cpu_cmd, capture_output=True, text=True, timeout=180)\n",
    "            print(\"   ‚úÖ PyTorch CPU version installed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå PyTorch installation failed: {e}\")\n",
    "    \n",
    "    # Step 2: Try to install CUDA-accelerated packages\n",
    "    print(\"\\n2Ô∏è‚É£ Installing CUDA-accelerated packages...\")\n",
    "    cuda_packages = [\n",
    "        ('cupy-cuda12x', 'GPU-accelerated NumPy'),\n",
    "        ('nvidia-ml-py3', 'GPU monitoring'),\n",
    "        ('gpustat', 'GPU statistics')\n",
    "    ]\n",
    "    \n",
    "    for package, description in cuda_packages:\n",
    "        try:\n",
    "            result = subprocess.run([\n",
    "                sys.executable, '-m', 'pip', 'install', package\n",
    "            ], capture_output=True, text=True, timeout=120)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                print(f\"   ‚úÖ {package} ({description})\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è {package} failed - GPU not available or incompatible\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå {package}: {e}\")\n",
    "    \n",
    "    # Step 3: Install core packages from requirements\n",
    "    print(\"\\n3Ô∏è‚É£ Installing core ML packages...\")\n",
    "    try:\n",
    "        result = subprocess.run([\n",
    "            sys.executable, '-m', 'pip', 'install', '-r', 'enhanced_requirements_gpu.txt'\n",
    "        ], capture_output=True, text=True, timeout=400)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"   ‚úÖ Core packages installed successfully!\")\n",
    "            # Show last few lines of successful installation\n",
    "            output_lines = result.stdout.split('\\n')[-5:]\n",
    "            for line in output_lines:\n",
    "                if line.strip():\n",
    "                    print(f\"      {line}\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Some packages failed, checking individual installs...\")\n",
    "            # Try essential packages individually\n",
    "            essential_packages = [\n",
    "                'transformers', 'sentence-transformers', 'scikit-learn',\n",
    "                'nltk', 'spacy', 'elasticsearch', 'neo4j', 'plotly',\n",
    "                'matplotlib', 'pandas', 'numpy', 'networkx'\n",
    "            ]\n",
    "            \n",
    "            for package in essential_packages:\n",
    "                try:\n",
    "                    subprocess.run([\n",
    "                        sys.executable, '-m', 'pip', 'install', package\n",
    "                    ], capture_output=True, text=True, timeout=60)\n",
    "                    print(f\"      ‚úÖ {package}\")\n",
    "                except:\n",
    "                    print(f\"      ‚ùå {package}\")\n",
    "                    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"   ‚è∞ Installation timed out (>6 minutes)\")\n",
    "    except Exception as e:\n",
    "        print(f\"   üí• Installation crashed: {e}\")\n",
    "\n",
    "# Run the GPU-optimized installation\n",
    "install_gpu_packages()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéÆ GPU OPTIMIZATION STATUS:\")\n",
    "\n",
    "# Check if GPU packages are available\n",
    "gpu_status = {}\n",
    "try:\n",
    "    import torch\n",
    "    gpu_status['torch_cuda'] = torch.cuda.is_available()\n",
    "    print(f\"   PyTorch CUDA: {'‚úÖ Available' if gpu_status['torch_cuda'] else '‚ùå Not available'}\")\n",
    "except:\n",
    "    gpu_status['torch_cuda'] = False\n",
    "    print(\"   PyTorch CUDA: ‚ùå Not installed\")\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "    gpu_status['cupy'] = True\n",
    "    print(\"   CuPy (GPU NumPy): ‚úÖ Installed\")\n",
    "except:\n",
    "    gpu_status['cupy'] = False\n",
    "    print(\"   CuPy (GPU NumPy): ‚ùå Not available\")\n",
    "\n",
    "try:\n",
    "    import nvidia_ml_py3\n",
    "    gpu_status['nvidia_ml'] = True\n",
    "    print(\"   NVIDIA-ML: ‚úÖ Installed\")\n",
    "except:\n",
    "    gpu_status['nvidia_ml'] = False\n",
    "    print(\"   NVIDIA-ML: ‚ùå Not available\")\n",
    "\n",
    "# Overall GPU readiness\n",
    "gpu_ready = any(gpu_status.values())\n",
    "print(f\"\\nüöÄ GPU ACCELERATION: {'‚úÖ READY' if gpu_ready else '‚ö†Ô∏è CPU FALLBACK MODE'}\")\n",
    "\n",
    "if gpu_ready:\n",
    "    print(\"   üî• Your system is configured for maximum GPU performance!\")\n",
    "else:\n",
    "    print(\"   üí° System will run on CPU with optimized performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd7274",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Setup Machine Learning Models\n",
    "\n",
    "Now let's install and configure the required NLP models and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc15bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"üß† SETTING UP GPU-ACCELERATED MACHINE LEARNING MODELS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# 1. Install spaCy with GPU support\n",
    "print(\"1Ô∏è‚É£ Installing spaCy with GPU acceleration...\")\n",
    "try:\n",
    "    # Try to install spaCy with CUDA support first\n",
    "    result = subprocess.run([\n",
    "        sys.executable, '-m', 'pip', 'install', 'spacy[cuda121]'\n",
    "    ], capture_output=True, text=True, timeout=180)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"   ‚úÖ spaCy with CUDA support installed!\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è CUDA spaCy failed, installing standard version...\")\n",
    "        subprocess.run([\n",
    "            sys.executable, '-m', 'pip', 'install', 'spacy'\n",
    "        ], capture_output=True, text=True, timeout=120)\n",
    "        print(\"   ‚úÖ Standard spaCy installed\")\n",
    "    \n",
    "    # Download English model\n",
    "    print(\"   üì• Downloading en_core_web_sm model...\")\n",
    "    result = subprocess.run([\n",
    "        sys.executable, '-m', 'spacy', 'download', 'en_core_web_sm'\n",
    "    ], capture_output=True, text=True, timeout=120)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"   ‚úÖ spaCy en_core_web_sm model installed successfully!\")\n",
    "    else:\n",
    "        print(\"   ‚ùå spaCy model installation failed!\")\n",
    "        print(f\"   Error: {result.stderr[:200]}\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"   ‚è∞ spaCy installation timed out\")\n",
    "except Exception as e:\n",
    "    print(f\"   üí• spaCy installation crashed: {e}\")\n",
    "\n",
    "# 2. Download NLTK data\n",
    "print(\"\\n2Ô∏è‚É£ Downloading NLTK data...\")\n",
    "try:\n",
    "    import nltk\n",
    "    \n",
    "    # Download required datasets\n",
    "    datasets = [\n",
    "        'punkt', 'averaged_perceptron_tagger', 'maxent_ne_chunker',\n",
    "        'words', 'stopwords', 'wordnet', 'omw-1.4'\n",
    "    ]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        try:\n",
    "            nltk.download(dataset, quiet=True)\n",
    "            print(f\"   ‚úÖ {dataset}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è {dataset}: {e}\")\n",
    "    \n",
    "    print(\"   üéâ NLTK data download completed!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"   ‚ùå NLTK not available\")\n",
    "except Exception as e:\n",
    "    print(f\"   üí• NLTK setup failed: {e}\")\n",
    "\n",
    "# 3. Test GPU-accelerated sentence transformers\n",
    "print(\"\\n3Ô∏è‚É£ Setting up GPU-accelerated sentence transformers...\")\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    import torch\n",
    "    \n",
    "    # Choose device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"   üéØ Using device: {device}\")\n",
    "    \n",
    "    # Test with a small model optimized for GPU\n",
    "    print(\"   üîÑ Loading GPU-optimized sentence transformer...\")\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Test encoding with GPU optimization\n",
    "    test_texts = [\n",
    "        \"This is a test sentence for GPU acceleration.\",\n",
    "        \"The enhanced knowledge graph system uses CUDA optimization.\",\n",
    "        \"Machine learning models benefit from parallel GPU processing.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"   üöÄ Testing GPU-accelerated encoding...\")\n",
    "    \n",
    "    # Encode with timing\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode(test_texts, batch_size=32, show_progress_bar=False)\n",
    "    encoding_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"   ‚úÖ Model loaded successfully on {device}!\")\n",
    "    print(f\"   üìä Embedding shape: {embeddings.shape}\")\n",
    "    print(f\"   ‚ö° Encoding time: {encoding_time:.4f} seconds\")\n",
    "    print(f\"   üí° Model device: {model.device}\")\n",
    "    \n",
    "    # Test batch processing capabilities\n",
    "    if device == 'cuda':\n",
    "        print(\"   üî• Testing GPU batch processing...\")\n",
    "        large_batch = [\"Test sentence\"] * 100\n",
    "        \n",
    "        start_time = time.time()\n",
    "        batch_embeddings = model.encode(large_batch, batch_size=64, show_progress_bar=False)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   üöÄ Batch processing (100 texts): {batch_time:.4f} seconds\")\n",
    "        print(f\"   üìà Throughput: {100/batch_time:.1f} texts/second\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"   ‚ùå Sentence transformers not available\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Sentence transformers test failed: {e}\")\n",
    "\n",
    "# 4. Install additional GPU-accelerated NLP models\n",
    "print(\"\\n4Ô∏è‚É£ Installing additional GPU-accelerated models...\")\n",
    "\n",
    "gpu_models = {\n",
    "    'transformers': 'Hugging Face Transformers with GPU support',\n",
    "    'accelerate': 'Hugging Face acceleration library',\n",
    "    'optimum': 'Hardware-optimized transformers'\n",
    "}\n",
    "\n",
    "for package, description in gpu_models.items():\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            sys.executable, '-m', 'pip', 'install', package\n",
    "        ], capture_output=True, text=True, timeout=120)\n",
    "        print(f\"   ‚úÖ {package}: {description}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è {package}: Installation failed\")\n",
    "\n",
    "# 5. Test GPU memory optimization\n",
    "print(\"\\n5Ô∏è‚É£ Testing GPU memory optimization...\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"   üßπ Clearing GPU cache...\")\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Get memory info\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        print(f\"   üìä GPU Memory - Allocated: {allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Test memory allocation and cleanup\n",
    "        print(\"   üß™ Testing memory management...\")\n",
    "        test_tensor = torch.randn(1000, 1000, device='cuda')\n",
    "        allocated_after = torch.cuda.memory_allocated() / 1024**3\n",
    "        \n",
    "        del test_tensor\n",
    "        torch.cuda.empty_cache()\n",
    "        allocated_clean = torch.cuda.memory_allocated() / 1024**3\n",
    "        \n",
    "        print(f\"   ‚úÖ Memory test: {allocated_after:.2f} GB ‚Üí {allocated_clean:.2f} GB\")\n",
    "        print(\"   üéØ GPU memory management working correctly!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è GPU memory test failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üöÄ GPU-ACCELERATED ML SETUP COMPLETE!\")\n",
    "\n",
    "# Final verification\n",
    "verification_results = {}\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    verification_results['spacy'] = True\n",
    "    print(\"‚úÖ spaCy with en_core_web_sm: Ready\")\n",
    "except:\n",
    "    verification_results['spacy'] = False\n",
    "    print(\"‚ùå spaCy: Not ready\")\n",
    "\n",
    "try:\n",
    "    import nltk\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    verification_results['nltk'] = True\n",
    "    print(\"‚úÖ NLTK: Ready\")\n",
    "except:\n",
    "    verification_results['nltk'] = False\n",
    "    print(\"‚ùå NLTK: Not ready\")\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "    verification_results['sentence_transformers'] = True\n",
    "    print(f\"‚úÖ Sentence Transformers on {device}: Ready\")\n",
    "except:\n",
    "    verification_results['sentence_transformers'] = False\n",
    "    print(\"‚ùå Sentence Transformers: Not ready\")\n",
    "\n",
    "ready_count = sum(verification_results.values())\n",
    "total_count = len(verification_results)\n",
    "\n",
    "print(f\"\\nüéØ ML MODELS STATUS: {ready_count}/{total_count} ready\")\n",
    "\n",
    "if ready_count == total_count:\n",
    "    print(\"üî• ALL GPU-OPTIMIZED ML MODELS ARE READY!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Some models need attention - check the issues above\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63f466",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Configure and Start Services\n",
    "\n",
    "Let's check the status of required services and provide setup instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9fe5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "print(\"üîç CHECKING REQUIRED SERVICES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def check_service(name, url, timeout=5):\n",
    "    \"\"\"Check if a service is running\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=timeout)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"‚úÖ {name}: Running\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è {name}: Responded with status {response.status_code}\")\n",
    "            return False\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå {name}: Not running ({str(e)[:50]}...)\")\n",
    "        return False\n",
    "\n",
    "# Check services\n",
    "services = {\n",
    "    'Elasticsearch': 'http://localhost:9200',\n",
    "    'Ollama': 'http://localhost:11434/api/tags'\n",
    "}\n",
    "\n",
    "all_services_running = True\n",
    "\n",
    "for service_name, service_url in services.items():\n",
    "    status = check_service(service_name, service_url)\n",
    "    if not status:\n",
    "        all_services_running = False\n",
    "\n",
    "# Check Neo4j separately (different protocol)\n",
    "print(\"\\nüîç Checking Neo4j...\")\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"knowledge123\"))\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    driver.close()\n",
    "    print(\"‚úÖ Neo4j: Running with correct credentials\")\n",
    "    neo4j_running = True\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Neo4j: Not running or incorrect credentials ({str(e)[:50]}...)\")\n",
    "    neo4j_running = False\n",
    "    all_services_running = False\n",
    "\n",
    "# Check Ollama model specifically\n",
    "print(\"\\nü§ñ Checking Ollama qwen3:4b model...\")\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags', timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        model_names = [model.get('name', '') for model in models]\n",
    "        \n",
    "        if any('qwen3:4b' in name for name in model_names):\n",
    "            print(\"‚úÖ qwen3:4b model: Available\")\n",
    "            qwen_available = True\n",
    "        else:\n",
    "            print(\"‚ùå qwen3:4b model: Not found\")\n",
    "            print(f\"   Available models: {[name for name in model_names]}\")\n",
    "            qwen_available = False\n",
    "    else:\n",
    "        print(\"‚ùå Could not check Ollama models\")\n",
    "        qwen_available = False\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama model check failed: {e}\")\n",
    "    qwen_available = False\n",
    "\n",
    "print(f\"\\nüìä SERVICE STATUS SUMMARY:\")\n",
    "print(f\"   Elasticsearch: {'‚úÖ' if check_service('', 'http://localhost:9200', 2) else '‚ùå'}\")\n",
    "print(f\"   Neo4j: {'‚úÖ' if neo4j_running else '‚ùå'}\")  \n",
    "print(f\"   Ollama: {'‚úÖ' if check_service('', 'http://localhost:11434/api/tags', 2) else '‚ùå'}\")\n",
    "print(f\"   qwen3:4b model: {'‚úÖ' if qwen_available else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d5cb7",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Service Setup Instructions\n",
    "\n",
    "If any services are not running, here are the setup commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f855dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è SERVICE SETUP COMMANDS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check which services need setup\n",
    "services_to_setup = []\n",
    "\n",
    "# Re-check services quickly\n",
    "try:\n",
    "    es_ok = requests.get('http://localhost:9200', timeout=2).status_code == 200\n",
    "except:\n",
    "    es_ok = False\n",
    "    \n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"knowledge123\"))\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    driver.close()\n",
    "    neo4j_ok = True\n",
    "except:\n",
    "    neo4j_ok = False\n",
    "\n",
    "try:\n",
    "    response = requests.get('http://localhost:11434/api/tags', timeout=2)\n",
    "    ollama_ok = response.status_code == 200\n",
    "    if ollama_ok:\n",
    "        models = response.json().get('models', [])\n",
    "        qwen_ok = any('qwen3:4b' in model.get('name', '') for model in models)\n",
    "    else:\n",
    "        qwen_ok = False\n",
    "except:\n",
    "    ollama_ok = False\n",
    "    qwen_ok = False\n",
    "\n",
    "print(\"üìã WHAT YOU NEED TO DO:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "if not es_ok:\n",
    "    print(\"\\n1Ô∏è‚É£ START ELASTICSEARCH:\")\n",
    "    print(\"   üíª Command: docker-compose -f docker-compose-elasticsearch.yml up -d\")\n",
    "    print(\"   üåê Verify: http://localhost:9200\")\n",
    "\n",
    "if not neo4j_ok:\n",
    "    print(\"\\n2Ô∏è‚É£ START NEO4J:\")\n",
    "    print(\"   Option A - Neo4j Desktop:\")\n",
    "    print(\"   ‚Ä¢ Download from: https://neo4j.com/download/\")\n",
    "    print(\"   ‚Ä¢ Create database with password: knowledge123\")\n",
    "    print(\"   \")\n",
    "    print(\"   Option B - Docker:\")\n",
    "    print(\"   üíª Command: docker run -d -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/knowledge123 neo4j\")\n",
    "    print(\"   üåê Verify: http://localhost:7474\")\n",
    "\n",
    "if not ollama_ok:\n",
    "    print(\"\\n3Ô∏è‚É£ START OLLAMA:\")\n",
    "    print(\"   ‚Ä¢ Download from: https://ollama.ai/\")\n",
    "    print(\"   üíª Start: ollama serve\")\n",
    "\n",
    "if not qwen_ok and ollama_ok:\n",
    "    print(\"\\n4Ô∏è‚É£ INSTALL QWEN3:4B MODEL:\")\n",
    "    print(\"   üíª Command: ollama pull qwen3:4b\")\n",
    "    print(\"   ‚è±Ô∏è This may take 2-5 minutes depending on your internet\")\n",
    "\n",
    "if es_ok and neo4j_ok and ollama_ok and qwen_ok:\n",
    "    print(\"\\nüéâ ALL SERVICES ARE READY!\")\n",
    "    print(\"   You can proceed to the next section.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Please complete the setup steps above, then re-run the service check.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2009841",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Initialize System Directories\n",
    "\n",
    "Let's create all necessary directories for the knowledge graph system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üìÅ INITIALIZING SYSTEM DIRECTORIES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define required directories\n",
    "directories = {\n",
    "    'gpu_knowledge_graph_data': 'Knowledge graph data storage',\n",
    "    'enhanced_visualizations': 'Generated visualizations and reports',\n",
    "    'logs': 'System logs and debug information',\n",
    "    'temp_data': 'Temporary processing files',\n",
    "    'models_cache': 'Cached ML models'\n",
    "}\n",
    "\n",
    "created_dirs = []\n",
    "existing_dirs = []\n",
    "\n",
    "for dir_name, description in directories.items():\n",
    "    dir_path = Path(dir_name)\n",
    "    \n",
    "    if dir_path.exists():\n",
    "        existing_dirs.append(dir_name)\n",
    "        print(f\"‚úÖ {dir_name}/ (already exists)\")\n",
    "    else:\n",
    "        try:\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            created_dirs.append(dir_name)\n",
    "            print(f\"üÜï {dir_name}/ (created)\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {dir_name}/ (failed: {e})\")\n",
    "\n",
    "# Check data_large directory\n",
    "data_dir = Path('data_large')\n",
    "if data_dir.exists():\n",
    "    file_count = len(list(data_dir.glob('*')))\n",
    "    print(f\"‚úÖ data_large/ (exists with {file_count} files)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è data_large/ (missing - you'll need to add your documents here)\")\n",
    "\n",
    "print(f\"\\nüìä DIRECTORY SUMMARY:\")\n",
    "print(f\"   Created: {len(created_dirs)} directories\")\n",
    "print(f\"   Existing: {len(existing_dirs)} directories\")\n",
    "\n",
    "if created_dirs:\n",
    "    print(f\"   New directories: {', '.join(created_dirs)}\")\n",
    "\n",
    "# Set up basic configuration\n",
    "config_data = {\n",
    "    'elasticsearch_index': 'gpu_chapter_knowledge_v1',\n",
    "    'neo4j_database': 'enhanced_knowledge_graph',\n",
    "    'relations_index': 'enhanced_concept_relations_v1',\n",
    "    'setup_timestamp': str(Path(__file__).stat().st_mtime if Path(__file__).exists() else 'unknown'),\n",
    "    'python_version': sys.version,\n",
    "    'created_directories': created_dirs\n",
    "}\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "with open('system_config.json', 'w') as f:\n",
    "    json.dump(config_data, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Created system_config.json\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e122fe5",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Verify System Components\n",
    "\n",
    "Let's run a comprehensive verification of all system components to ensure everything is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b292e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç COMPREHENSIVE GPU-OPTIMIZED SYSTEM VERIFICATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "verification_results = {\n",
    "    'gpu_packages': {},\n",
    "    'cpu_fallback_packages': {},\n",
    "    'ml_models': {},\n",
    "    'services': {},\n",
    "    'system_files': {},\n",
    "    'gpu_performance': {}\n",
    "}\n",
    "\n",
    "# 1. Verify GPU-specific packages\n",
    "print(\"1Ô∏è‚É£ GPU-OPTIMIZED PACKAGES:\")\n",
    "gpu_packages = [\n",
    "    ('torch', 'PyTorch with CUDA'),\n",
    "    ('cupy', 'GPU-accelerated NumPy'),\n",
    "    ('nvidia_ml_py3', 'NVIDIA GPU monitoring'),\n",
    "    ('transformers', 'Hugging Face Transformers'),\n",
    "    ('accelerate', 'Model acceleration library')\n",
    "]\n",
    "\n",
    "for package, description in gpu_packages:\n",
    "    try:\n",
    "        module = __import__(package.replace('-', '_'))\n",
    "        print(f\"   ‚úÖ {package}: {description}\")\n",
    "        verification_results['gpu_packages'][package] = True\n",
    "        \n",
    "        # Special checks for key packages\n",
    "        if package == 'torch':\n",
    "            import torch\n",
    "            cuda_available = torch.cuda.is_available()\n",
    "            print(f\"      üéÆ CUDA: {'Available' if cuda_available else 'Not available'}\")\n",
    "            if cuda_available:\n",
    "                print(f\"      üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "                print(f\"      üìä Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "                \n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package}: {description}\")\n",
    "        verification_results['gpu_packages'][package] = False\n",
    "\n",
    "# 2. Verify CPU fallback packages\n",
    "print(\"\\n2Ô∏è‚É£ CORE ML PACKAGES (CPU FALLBACK):\")\n",
    "cpu_packages = [\n",
    "    ('sklearn', 'Scikit-learn'),\n",
    "    ('pandas', 'Data manipulation'),\n",
    "    ('numpy', 'Numerical computing'),\n",
    "    ('networkx', 'Graph processing'),\n",
    "    ('plotly', 'Interactive visualization'),\n",
    "    ('matplotlib', 'Static visualization')\n",
    "]\n",
    "\n",
    "for package, description in cpu_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"   ‚úÖ {package}: {description}\")\n",
    "        verification_results['cpu_fallback_packages'][package] = True\n",
    "    except ImportError:\n",
    "        print(f\"   ‚ùå {package}: {description}\")\n",
    "        verification_results['cpu_fallback_packages'][package] = False\n",
    "\n",
    "# 3. Verify GPU-accelerated ML models\n",
    "print(\"\\n3Ô∏è‚É£ GPU-ACCELERATED ML MODELS:\")\n",
    "try:\n",
    "    import torch\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # spaCy with GPU awareness\n",
    "    try:\n",
    "        import spacy\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Test if spaCy can utilize GPU (if available)\n",
    "        if torch.cuda.is_available():\n",
    "            try:\n",
    "                # Check if spaCy has GPU components\n",
    "                has_gpu_components = any('gpu' in str(pipe).lower() for pipe in nlp.pipeline)\n",
    "                print(f\"   ‚úÖ spaCy en_core_web_sm (GPU-aware: {has_gpu_components})\")\n",
    "            except:\n",
    "                print(f\"   ‚úÖ spaCy en_core_web_sm (CPU mode)\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ spaCy en_core_web_sm (CPU mode)\")\n",
    "        verification_results['ml_models']['spacy'] = True\n",
    "    except:\n",
    "        print(f\"   ‚ùå spaCy en_core_web_sm\")\n",
    "        verification_results['ml_models']['spacy'] = False\n",
    "    \n",
    "    # NLTK\n",
    "    try:\n",
    "        import nltk\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "        print(f\"   ‚úÖ NLTK punkt\")\n",
    "        verification_results['ml_models']['nltk'] = True\n",
    "    except:\n",
    "        print(f\"   ‚ùå NLTK punkt\")\n",
    "        verification_results['ml_models']['nltk'] = False\n",
    "    \n",
    "    # Sentence Transformers with GPU\n",
    "    try:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "        \n",
    "        # Test GPU acceleration\n",
    "        test_text = [\"GPU acceleration test for sentence transformers\"]\n",
    "        embeddings = model.encode(test_text)\n",
    "        \n",
    "        print(f\"   ‚úÖ Sentence Transformers (device: {device})\")\n",
    "        print(f\"      üìä Model device: {model.device}\")\n",
    "        print(f\"      üéØ Embedding shape: {embeddings.shape}\")\n",
    "        verification_results['ml_models']['sentence_transformers'] = True\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Sentence Transformers: {str(e)[:50]}\")\n",
    "        verification_results['ml_models']['sentence_transformers'] = False\n",
    "\n",
    "except ImportError:\n",
    "    print(\"   ‚ùå PyTorch not available for device detection\")\n",
    "\n",
    "# 4. Verify services with performance expectations\n",
    "print(\"\\n4Ô∏è‚É£ SERVICES (WITH GPU AWARENESS):\")\n",
    "services_to_check = [\n",
    "    ('Elasticsearch', 'http://localhost:9200'),\n",
    "    ('Ollama', 'http://localhost:11434/api/tags')\n",
    "]\n",
    "\n",
    "for service_name, service_url in services_to_check:\n",
    "    try:\n",
    "        response = requests.get(service_url, timeout=3)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"   ‚úÖ {service_name}: Running\")\n",
    "            verification_results['services'][service_name.lower()] = True\n",
    "            \n",
    "            # Special check for Ollama GPU usage\n",
    "            if service_name == 'Ollama':\n",
    "                try:\n",
    "                    models_response = response.json()\n",
    "                    models = models_response.get('models', [])\n",
    "                    qwen_models = [m for m in models if 'qwen' in m.get('name', '').lower()]\n",
    "                    if qwen_models:\n",
    "                        print(f\"      ü§ñ qwen models available: {len(qwen_models)}\")\n",
    "                        print(f\"      üí° Ollama can utilize GPU for inference\")\n",
    "                except:\n",
    "                    pass\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è {service_name}: HTTP {response.status_code}\")\n",
    "            verification_results['services'][service_name.lower()] = False\n",
    "    except:\n",
    "        print(f\"   ‚ùå {service_name}: Not running\")\n",
    "        verification_results['services'][service_name.lower()] = False\n",
    "\n",
    "# Neo4j check\n",
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"knowledge123\"))\n",
    "    with driver.session() as session:\n",
    "        session.run(\"RETURN 1\")\n",
    "    driver.close()\n",
    "    print(f\"   ‚úÖ Neo4j: Running\")\n",
    "    verification_results['services']['neo4j'] = True\n",
    "except:\n",
    "    print(f\"   ‚ùå Neo4j: Not running\")\n",
    "    verification_results['services']['neo4j'] = False\n",
    "\n",
    "# 5. GPU Performance benchmarking\n",
    "print(\"\\n5Ô∏è‚É£ GPU PERFORMANCE BENCHMARKING:\")\n",
    "try:\n",
    "    import torch\n",
    "    import time\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"   üöÄ Running GPU performance tests...\")\n",
    "        \n",
    "        # Matrix multiplication benchmark\n",
    "        size = 2000\n",
    "        cpu_tensor = torch.randn(size, size)\n",
    "        gpu_tensor = torch.randn(size, size).cuda()\n",
    "        \n",
    "        # CPU benchmark\n",
    "        start_time = time.time()\n",
    "        cpu_result = torch.mm(cpu_tensor, cpu_tensor)\n",
    "        cpu_time = time.time() - start_time\n",
    "        \n",
    "        # GPU benchmark\n",
    "        torch.cuda.synchronize()\n",
    "        start_time = time.time()\n",
    "        gpu_result = torch.mm(gpu_tensor, gpu_tensor)\n",
    "        torch.cuda.synchronize()\n",
    "        gpu_time = time.time() - start_time\n",
    "        \n",
    "        speedup = cpu_time / gpu_time\n",
    "        \n",
    "        print(f\"   üìä Matrix multiplication ({size}x{size}):\")\n",
    "        print(f\"      CPU: {cpu_time:.4f}s\")\n",
    "        print(f\"      GPU: {gpu_time:.4f}s\")\n",
    "        print(f\"      üéØ Speedup: {speedup:.1f}x\")\n",
    "        \n",
    "        verification_results['gpu_performance']['matrix_speedup'] = speedup\n",
    "        \n",
    "        # Memory bandwidth test\n",
    "        print(\"   üß† GPU memory bandwidth test...\")\n",
    "        memory_size = 100 * 1024 * 1024  # 100MB\n",
    "        data = torch.randn(memory_size // 4).cuda()  # float32 = 4 bytes\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result = data * 2.0  # Simple operation to test memory bandwidth\n",
    "        torch.cuda.synchronize()\n",
    "        memory_time = time.time() - start_time\n",
    "        \n",
    "        bandwidth = (memory_size * 2) / memory_time / 1024**3  # GB/s (read + write)\n",
    "        print(f\"      üöÄ Memory bandwidth: {bandwidth:.1f} GB/s\")\n",
    "        verification_results['gpu_performance']['memory_bandwidth'] = bandwidth\n",
    "        \n",
    "        if speedup > 10:\n",
    "            print(\"   ‚úÖ EXCELLENT GPU performance!\")\n",
    "        elif speedup > 5:\n",
    "            print(\"   ‚úÖ GOOD GPU performance\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è LIMITED GPU benefit\")\n",
    "            \n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No GPU available - CPU performance mode\")\n",
    "        verification_results['gpu_performance']['gpu_available'] = False\n",
    "        \n",
    "        # CPU multi-threading test\n",
    "        print(\"   üîÑ Testing CPU multi-threading capability...\")\n",
    "        size = 1000\n",
    "        cpu_tensor = torch.randn(size, size)\n",
    "        \n",
    "        # Single thread\n",
    "        torch.set_num_threads(1)\n",
    "        start_time = time.time()\n",
    "        result1 = torch.mm(cpu_tensor, cpu_tensor)\n",
    "        single_time = time.time() - start_time\n",
    "        \n",
    "        # Multi thread\n",
    "        torch.set_num_threads(torch.get_num_threads())\n",
    "        start_time = time.time()\n",
    "        result2 = torch.mm(cpu_tensor, cpu_tensor)\n",
    "        multi_time = time.time() - start_time\n",
    "        \n",
    "        threading_speedup = single_time / multi_time\n",
    "        print(f\"      üßµ CPU threading speedup: {threading_speedup:.1f}x\")\n",
    "        verification_results['gpu_performance']['cpu_threading_speedup'] = threading_speedup\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Performance test failed: {e}\")\n",
    "\n",
    "# 6. Calculate overall GPU readiness\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"üìä GPU-OPTIMIZED SYSTEM READINESS REPORT\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "categories = {\n",
    "    'GPU Packages': verification_results['gpu_packages'],\n",
    "    'CPU Fallback': verification_results['cpu_fallback_packages'],\n",
    "    'ML Models': verification_results['ml_models'],\n",
    "    'Services': verification_results['services']\n",
    "}\n",
    "\n",
    "overall_ready = True\n",
    "gpu_optimized = False\n",
    "\n",
    "for category, results in categories.items():\n",
    "    passed = sum(results.values())\n",
    "    total = len(results)\n",
    "    percentage = (passed / total * 100) if total > 0 else 0\n",
    "    \n",
    "    if category == 'GPU Packages' and passed > 0:\n",
    "        gpu_optimized = True\n",
    "    \n",
    "    status = \"‚úÖ\" if percentage >= 80 else \"‚ö†Ô∏è\" if percentage >= 60 else \"‚ùå\"\n",
    "    print(f\"{status} {category}: {passed}/{total} ({percentage:.0f}%)\")\n",
    "    \n",
    "    if percentage < 60:\n",
    "        overall_ready = False\n",
    "\n",
    "# GPU acceleration status\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available() and gpu_optimized:\n",
    "        print(f\"\\nüî• GPU ACCELERATION: FULLY ENABLED\")\n",
    "        print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    elif gpu_optimized:\n",
    "        print(f\"\\n‚ö° GPU PACKAGES: INSTALLED (No GPU hardware)\")\n",
    "    else:\n",
    "        print(f\"\\nüíª CPU MODE: Optimized for CPU processing\")\n",
    "except:\n",
    "    print(f\"\\nüíª CPU MODE: Standard configuration\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "if overall_ready and gpu_optimized:\n",
    "    print(\"üöÄ SYSTEM IS GPU-OPTIMIZED AND READY!\")\n",
    "    print(\"   Maximum performance configuration achieved\")\n",
    "elif overall_ready:\n",
    "    print(\"‚úÖ SYSTEM IS READY (CPU-optimized mode)\")\n",
    "    print(\"   Will run efficiently on available hardware\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è SYSTEM NEEDS ATTENTION\")\n",
    "    print(\"   Please fix the issues above before proceeding\")\n",
    "    \n",
    "print(\"\\nüí° PERFORMANCE EXPECTATIONS:\")\n",
    "if gpu_optimized and torch.cuda.is_available() if 'torch' in locals() else False:\n",
    "    print(\"   üî• GPU Mode: 5-20x faster processing\")\n",
    "    print(\"   üìä Batch sizes: 64-128 recommended\")\n",
    "    print(\"   ‚ö° Memory usage: Optimized for GPU\")\n",
    "else:\n",
    "    print(\"   üíª CPU Mode: Optimized multi-threading\")\n",
    "    print(\"   üìä Batch sizes: 8-16 recommended\")\n",
    "    print(\"   üß† Memory usage: System RAM optimized\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d16da2",
   "metadata": {},
   "source": [
    "## üß™ Quick System Test\n",
    "\n",
    "Let's run a quick test of the enhanced knowledge graph system components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ff803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of GPU-optimized system components\n",
    "print(\"üß™ QUICK GPU-OPTIMIZED SYSTEM TEST\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Test 1: GPU-accelerated concept extraction\n",
    "print(\"1Ô∏è‚É£ Testing GPU-accelerated concept extraction...\")\n",
    "try:\n",
    "    import torch\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"   üéØ Using device: {device}\")\n",
    "    \n",
    "    # Simulate the dynamic concept extraction process with GPU acceleration\n",
    "    sample_text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on algorithms \n",
    "    that can learn from data. Before implementing neural networks, you must understand \n",
    "    linear algebra and statistics. Key concepts include supervised learning, \n",
    "    unsupervised learning, and reinforcement learning. CUDA acceleration enables \n",
    "    faster matrix operations and parallel processing for deep learning models.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test NLTK with timing\n",
    "    import time\n",
    "    import nltk\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sentences = sent_tokenize(sample_text)\n",
    "    nltk_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ NLTK tokenization: {len(sentences)} sentences ({nltk_time:.4f}s)\")\n",
    "    \n",
    "    # Test spaCy with GPU awareness\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    doc = nlp(sample_text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    spacy_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ spaCy NER: {len(entities)} entities ({spacy_time:.4f}s)\")\n",
    "    \n",
    "    # Test GPU-accelerated sentence transformers\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device=device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    embeddings = model.encode([sample_text], batch_size=1)\n",
    "    embedding_time = time.time() - start_time\n",
    "    print(f\"   ‚úÖ GPU Embeddings: shape {embeddings.shape} ({embedding_time:.4f}s)\")\n",
    "    print(f\"      üéÆ Model device: {model.device}\")\n",
    "    \n",
    "    # Test batch processing performance\n",
    "    if device == 'cuda':\n",
    "        print(\"   üöÄ Testing GPU batch processing...\")\n",
    "        test_sentences = sentences * 10  # Create larger batch\n",
    "        \n",
    "        start_time = time.time()\n",
    "        batch_embeddings = model.encode(test_sentences, batch_size=32, show_progress_bar=False)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        throughput = len(test_sentences) / batch_time\n",
    "        print(f\"      üìä Batch processing: {len(test_sentences)} texts in {batch_time:.4f}s\")\n",
    "        print(f\"      üéØ Throughput: {throughput:.1f} texts/second\")\n",
    "        \n",
    "        # Compare with CPU\n",
    "        cpu_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device='cpu')\n",
    "        start_time = time.time()\n",
    "        cpu_embeddings = cpu_model.encode(test_sentences[:10], batch_size=8, show_progress_bar=False)\n",
    "        cpu_time = time.time() - start_time\n",
    "        \n",
    "        if cpu_time > 0:\n",
    "            speedup = (cpu_time * len(test_sentences) / 10) / batch_time\n",
    "            print(f\"      üî• GPU vs CPU speedup: {speedup:.1f}x\")\n",
    "    else:\n",
    "        print(\"   üíª CPU mode: Testing multi-threading...\")\n",
    "        torch.set_num_threads(max(1, torch.get_num_threads()))\n",
    "        threading_info = f\"Using {torch.get_num_threads()} threads\"\n",
    "        print(f\"      üßµ {threading_info}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Concept extraction test failed: {e}\")\n",
    "\n",
    "# Test 2: GPU-aware database connections\n",
    "print(\"\\n2Ô∏è‚É£ Testing database connections with GPU awareness...\")\n",
    "try:\n",
    "    # Test Elasticsearch\n",
    "    from elasticsearch import Elasticsearch\n",
    "    es = Elasticsearch(['http://localhost:9200'])\n",
    "    es_info = es.info()\n",
    "    print(f\"   ‚úÖ Elasticsearch: {es_info['version']['number']}\")\n",
    "    \n",
    "    # Test if we can create GPU-optimized indices\n",
    "    try:\n",
    "        # Test index with GPU-optimized settings\n",
    "        gpu_index_settings = {\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 0,\n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"gpu_optimized\": {\n",
    "                            \"type\": \"standard\",\n",
    "                            \"stopwords\": \"_english_\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        print(f\"      üéÆ GPU-optimized index settings ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ö†Ô∏è Index optimization setup: {e}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Elasticsearch: {e}\")\n",
    "\n",
    "try:\n",
    "    # Test Neo4j with performance settings\n",
    "    from neo4j import GraphDatabase\n",
    "    driver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"knowledge123\"))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Test with a performance query\n",
    "        result = session.run(\"\"\"\n",
    "            CALL dbms.components() YIELD name, versions \n",
    "            RETURN name, versions[0] as version\n",
    "        \"\"\")\n",
    "        \n",
    "        for record in result:\n",
    "            print(f\"   ‚úÖ Neo4j: {record['version']}\")\n",
    "            break\n",
    "            \n",
    "        # Test graph algorithms readiness\n",
    "        try:\n",
    "            gds_result = session.run(\"CALL gds.version()\")\n",
    "            for record in gds_result:\n",
    "                print(f\"      üöÄ Graph Data Science: {record[0]}\")\n",
    "                break\n",
    "        except:\n",
    "            print(f\"      üí° Standard Neo4j (no GDS acceleration)\")\n",
    "            \n",
    "    driver.close()\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Neo4j: {e}\")\n",
    "\n",
    "# Test 3: GPU-optimized Ollama connection\n",
    "print(\"\\n3Ô∏è‚É£ Testing GPU-optimized Ollama connection...\")\n",
    "try:\n",
    "    import requests\n",
    "    response = requests.get('http://localhost:11434/api/tags', timeout=5)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        models = response.json().get('models', [])\n",
    "        qwen_models = [m for m in models if 'qwen' in m.get('name', '').lower()]\n",
    "        \n",
    "        if qwen_models:\n",
    "            print(f\"   ‚úÖ Ollama: {len(qwen_models)} qwen models available\")\n",
    "            \n",
    "            # Test GPU utilization for inference\n",
    "            test_prompt = {\n",
    "                \"model\": \"qwen3:4b\",\n",
    "                \"prompt\": \"What is GPU acceleration?\",\n",
    "                \"stream\": False\n",
    "            }\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                inference_response = requests.post(\n",
    "                    'http://localhost:11434/api/generate', \n",
    "                    json=test_prompt, \n",
    "                    timeout=10\n",
    "                )\n",
    "                inference_time = time.time() - start_time\n",
    "                \n",
    "                if inference_response.status_code == 200:\n",
    "                    response_data = inference_response.json()\n",
    "                    response_text = response_data.get('response', '')\n",
    "                    print(f\"      ü§ñ Inference test: {inference_time:.2f}s\")\n",
    "                    print(f\"      üí¨ Response length: {len(response_text)} chars\")\n",
    "                    \n",
    "                    # Check for GPU utilization hints\n",
    "                    if inference_time < 2.0:\n",
    "                        print(f\"      üöÄ Fast inference suggests GPU acceleration\")\n",
    "                    else:\n",
    "                        print(f\"      üíª Inference time suggests CPU mode\")\n",
    "                else:\n",
    "                    print(f\"      ‚ö†Ô∏è Inference test failed: HTTP {inference_response.status_code}\")\n",
    "                    \n",
    "            except requests.exceptions.Timeout:\n",
    "                print(f\"      ‚è∞ Inference test timed out (model loading?)\")\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Inference test error: {str(e)[:50]}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è Ollama: {len(models)} models, but no qwen models\")\n",
    "            print(f\"      üí° Available models: {[m.get('name', '') for m in models[:3]]}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå Ollama: HTTP {response.status_code}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Ollama: {e}\")\n",
    "\n",
    "# Test 4: GPU memory and performance monitoring\n",
    "print(\"\\n4Ô∏è‚É£ Testing GPU monitoring capabilities...\")\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"   üéÆ GPU monitoring active...\")\n",
    "        \n",
    "        # Memory monitoring\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        \n",
    "        print(f\"      üìä GPU Memory: {allocated:.2f}/{total_memory:.1f} GB allocated\")\n",
    "        print(f\"      üß† Reserved: {reserved:.2f} GB\")\n",
    "        \n",
    "        # Test memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        after_cleanup = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"      üßπ After cleanup: {after_cleanup:.2f} GB\")\n",
    "        \n",
    "        # GPU utilization test\n",
    "        try:\n",
    "            import nvidia_ml_py3 as nvml\n",
    "            nvml.nvmlInit()\n",
    "            handle = nvml.nvmlDeviceGetHandleByIndex(0)\n",
    "            \n",
    "            # Temperature\n",
    "            temp = nvml.nvmlDeviceGetTemperature(handle, nvml.NVML_TEMPERATURE_GPU)\n",
    "            print(f\"      üå°Ô∏è GPU Temperature: {temp}¬∞C\")\n",
    "            \n",
    "            # Utilization\n",
    "            util = nvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            print(f\"      ‚ö° GPU Utilization: {util.gpu}%\")\n",
    "            print(f\"      üß† Memory Utilization: {util.memory}%\")\n",
    "            \n",
    "        except ImportError:\n",
    "            print(f\"      üí° Install nvidia-ml-py3 for detailed GPU monitoring\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è GPU monitoring error: {str(e)[:50]}\")\n",
    "    else:\n",
    "        print(\"   üíª CPU monitoring active...\")\n",
    "        import psutil\n",
    "        \n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        memory = psutil.virtual_memory()\n",
    "        \n",
    "        print(f\"      üßÆ CPU Usage: {cpu_percent}%\")\n",
    "        print(f\"      üß† RAM Usage: {memory.percent}% ({memory.used/1024**3:.1f}/{memory.total/1024**3:.1f} GB)\")\n",
    "        print(f\"      üßµ CPU Threads: {torch.get_num_threads()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Monitoring test failed: {e}\")\n",
    "\n",
    "print(\"\\n\udfaf QUICK TEST COMPLETE!\")\n",
    "\n",
    "# Summary\n",
    "try:\n",
    "    import torch\n",
    "    device_info = f\"Device: {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"Device: CPU\"\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "except:\n",
    "    device_info = \"Device: Unknown\"\n",
    "    gpu_available = False\n",
    "\n",
    "print(f\"\\nüìä SYSTEM STATUS:\")\n",
    "print(f\"   {device_info}\")\n",
    "print(f\"   GPU Acceleration: {'‚úÖ ENABLED' if gpu_available else 'üíª CPU MODE'}\")\n",
    "print(f\"   Ready for Knowledge Graph: {'üöÄ YES' if gpu_available else '‚úÖ YES (CPU optimized)'}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"\\nüî• Your GPU-optimized system is ready for maximum performance!\")\n",
    "    print(f\"   Expected speedup: 5-20x faster than CPU-only systems\")\n",
    "else:\n",
    "    print(f\"\\nüí° Your CPU-optimized system is ready for efficient processing!\")\n",
    "    print(f\"   Multi-threading enabled for best CPU performance\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f8fbc",
   "metadata": {},
   "source": [
    "## üöÄ Run Complete Enhanced Knowledge Graph System\n",
    "\n",
    "Once all prerequisites are satisfied, you can run the complete system using one of these approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be50aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ GPU-OPTIMIZED ENHANCED KNOWLEDGE GRAPH SYSTEM - EXECUTION OPTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"üìã AVAILABLE GPU-ACCELERATED EXECUTION METHODS:\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ AUTOMATED GPU-OPTIMIZED TESTER:\")\n",
    "print(\"   üíª Command: python KG_SYSTEM_TESTER.py --gpu\")\n",
    "print(\"   üéØ What it does:\")\n",
    "print(\"     ‚Ä¢ Runs complete automated testing with GPU acceleration\")\n",
    "print(\"     ‚Ä¢ Benchmarks GPU vs CPU performance\")\n",
    "print(\"     ‚Ä¢ Tests CUDA memory management\")\n",
    "print(\"     ‚Ä¢ Validates GPU-optimized components\")\n",
    "print(\"     ‚Ä¢ Generates GPU performance reports\")\n",
    "print(\"     ‚Ä¢ Provides GPU-specific troubleshooting\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ COMPLETE GPU-ACCELERATED SYSTEM RUNNER:\")\n",
    "print(\"   üíª Command: python KG_ENHANCED_COMPLETE_RUNNER.py --gpu\")\n",
    "print(\"   üéØ What it does:\")\n",
    "print(\"     ‚Ä¢ Executes full pipeline with maximum GPU utilization\")\n",
    "print(\"     ‚Ä¢ Uses GPU-accelerated batch processing (batch_size=64)\")\n",
    "print(\"     ‚Ä¢ Leverages CUDA for embedding generation\")\n",
    "print(\"     ‚Ä¢ GPU-optimized clustering with CuML (if available)\")\n",
    "print(\"     ‚Ä¢ Fast GPU-based similarity calculations\")\n",
    "print(\"     ‚Ä¢ Interactive GPU-accelerated query system\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ STEP-BY-STEP GPU EXECUTION:\")\n",
    "print(\"   üíª Commands with GPU flags:\")\n",
    "print(\"     python KG_ENHANCED_1_build_chapter_database_gpu.py --use-gpu\")\n",
    "print(\"     python KG_ENHANCED_2_build_knowledge_graph_gpu.py --use-gpu\") \n",
    "print(\"     python KG_ENHANCED_3_query_knowledge_graph_gpu.py --use-gpu\")\n",
    "print(\"     python KG_ENHANCED_4_visualize_knowledge_graph_gpu.py --use-gpu\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ QUICK GPU TEST MODE:\")\n",
    "print(\"   üíª Command: python KG_ENHANCED_COMPLETE_RUNNER.py --gpu --quick\")\n",
    "print(\"   üéØ Uses limited data with full GPU acceleration for testing\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ GPU PERFORMANCE BENCHMARKING:\")\n",
    "print(\"   üíª Command: python KG_ENHANCED_COMPLETE_RUNNER.py --benchmark\")\n",
    "print(\"   üéØ Runs performance comparisons:\")\n",
    "print(\"     ‚Ä¢ GPU vs CPU processing times\")\n",
    "print(\"     ‚Ä¢ Memory usage optimization\")  \n",
    "print(\"     ‚Ä¢ Throughput measurements\")\n",
    "print(\"     ‚Ä¢ CUDA efficiency analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ RECOMMENDED GPU-OPTIMIZED APPROACH:\")\n",
    "print(\"   1. First: python KG_SYSTEM_TESTER.py --gpu\")\n",
    "print(\"   2. If GPU tests pass: python KG_ENHANCED_COMPLETE_RUNNER.py --gpu --quick\")\n",
    "print(\"   3. For full performance: python KG_ENHANCED_COMPLETE_RUNNER.py --gpu\")\n",
    "print(\"   4. For benchmarking: python KG_ENHANCED_COMPLETE_RUNNER.py --benchmark\")\n",
    "\n",
    "print(\"\\nüî• GPU-SPECIFIC PERFORMANCE FEATURES:\")\n",
    "print(\"   ‚Ä¢ CUDA-accelerated embeddings with batch processing\")\n",
    "print(\"   ‚Ä¢ GPU memory-optimized data structures\")\n",
    "print(\"   ‚Ä¢ Parallel concept extraction using GPU tensors\")\n",
    "print(\"   ‚Ä¢ Fast similarity search with GPU matrix operations\")\n",
    "print(\"   ‚Ä¢ GPU-accelerated K-means clustering\")\n",
    "print(\"   ‚Ä¢ Real-time GPU memory monitoring and optimization\")\n",
    "print(\"   ‚Ä¢ Automatic fallback to CPU if GPU unavailable\")\n",
    "\n",
    "print(\"\\nüìä EXPECTED GPU PERFORMANCE IMPROVEMENTS:\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"   üéÆ GPU: {gpu_name}\")\n",
    "        print(f\"   üß† VRAM: {gpu_memory:.1f} GB\")\n",
    "        print(f\"   üöÄ Expected speedup: 10-20x for embedding generation\")\n",
    "        print(f\"   ‚ö° Expected speedup: 5-15x for similarity calculations\")\n",
    "        print(f\"   üìä Recommended batch size: 64-128\")\n",
    "        print(f\"   üéØ Optimal memory usage: {min(8, gpu_memory * 0.8):.1f} GB\")\n",
    "    else:\n",
    "        print(f\"   üíª CPU Mode: Multi-threaded optimization\")\n",
    "        print(f\"   üßµ CPU threads: {torch.get_num_threads()}\")\n",
    "        print(f\"   üìä Recommended batch size: 8-16\")\n",
    "        print(f\"   üéØ Expected performance: Standard CPU speed\")\n",
    "except:\n",
    "    print(f\"   ‚ö†Ô∏è Performance metrics unavailable\")\n",
    "\n",
    "print(\"\\nüí° GPU OPTIMIZATION TIPS:\")\n",
    "print(\"   ‚Ä¢ Ensure CUDA 12.1+ drivers are installed\")\n",
    "print(\"   ‚Ä¢ Close other GPU-intensive applications\")\n",
    "print(\"   ‚Ä¢ Monitor GPU memory usage during processing\")\n",
    "print(\"   ‚Ä¢ Use --batch-size flag to optimize for your GPU\")\n",
    "print(\"   ‚Ä¢ Enable mixed precision with --fp16 for 2x speedup\")\n",
    "\n",
    "print(\"\\nüéõÔ∏è ADVANCED GPU CONFIGURATION:\")\n",
    "print(\"   Environment Variables:\")\n",
    "print(\"     CUDA_VISIBLE_DEVICES=0     # Use specific GPU\")\n",
    "print(\"     PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\")\n",
    "print(\"     OMP_NUM_THREADS=4          # CPU threads for hybrid processing\")\n",
    "\n",
    "print(\"\\nüìà PERFORMANCE MONITORING:\")\n",
    "print(\"   During execution, monitor:\")\n",
    "print(\"     ‚Ä¢ GPU utilization: nvidia-smi\")\n",
    "print(\"     ‚Ä¢ Memory usage: Built-in monitoring\")\n",
    "print(\"     ‚Ä¢ Temperature: Automatic thermal management\")\n",
    "print(\"     ‚Ä¢ Throughput: Real-time processing statistics\")\n",
    "\n",
    "print(\"\\nüîß GPU TROUBLESHOOTING FLAGS:\")\n",
    "print(\"   --cpu-fallback     Force CPU mode if GPU issues\")\n",
    "print(\"   --small-batch      Use batch_size=8 for limited VRAM\")\n",
    "print(\"   --fp16             Enable half-precision for memory savings\")\n",
    "print(\"   --memory-fraction  Limit GPU memory usage (0.5 = 50%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Real-time GPU status\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        print(\"üéÆ CURRENT GPU STATUS:\")\n",
    "        print(f\"   Memory: {allocated:.2f}/{total:.1f} GB allocated\")\n",
    "        print(f\"   Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"   Available: {total - reserved:.1f} GB for processing\")\n",
    "        \n",
    "        if total - reserved > 4:\n",
    "            print(\"   ‚úÖ Sufficient GPU memory for large-scale processing\")\n",
    "        elif total - reserved > 2:\n",
    "            print(\"   ‚úÖ Adequate GPU memory for medium-scale processing\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è Limited GPU memory - consider --small-batch flag\")\n",
    "    else:\n",
    "        import psutil\n",
    "        ram = psutil.virtual_memory()\n",
    "        print(\"üíª CURRENT CPU STATUS:\")\n",
    "        print(f\"   RAM: {ram.used/1024**3:.1f}/{ram.total/1024**3:.1f} GB\")\n",
    "        print(f\"   Available: {ram.available/1024**3:.1f} GB\")\n",
    "        print(f\"   CPU threads: {torch.get_num_threads()}\")\n",
    "except:\n",
    "    print(\"üìä System status check unavailable\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8161a05b",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Troubleshooting Common Issues\n",
    "\n",
    "Based on your setup output, here are solutions to common problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeae20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "print(\"‚ùå PROBLEM: torch-audio installation failed\")\n",
    "print(\"‚úÖ SOLUTION: Use the fixed requirements file (already created above)\")\n",
    "print(\"   The torch-audio package is incompatible with Python 3.12\")\n",
    "print(\"   Our fixed version removes this problematic dependency\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: spaCy model installation failed\")  \n",
    "print(\"‚úÖ SOLUTION: Install manually:\")\n",
    "print(\"   Command: python -m spacy download en_core_web_sm\")\n",
    "print(\"   Alternative: pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: Neo4j not running\")\n",
    "print(\"‚úÖ SOLUTION: Start Neo4j service:\")\n",
    "print(\"   Option 1 - Docker: docker run -d -p 7474:7474 -p 7687:7687 -e NEO4J_AUTH=neo4j/knowledge123 neo4j\")\n",
    "print(\"   Option 2 - Desktop: Download from https://neo4j.com/download/\")\n",
    "print(\"   Remember: Password must be 'knowledge123'\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: qwen3:4b model not found\")\n",
    "print(\"‚úÖ SOLUTION: Install Ollama model:\")\n",
    "print(\"   Command: ollama pull qwen3:4b\")\n",
    "print(\"   Note: This downloads ~2.5GB, may take 2-5 minutes\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: No GPU available\")\n",
    "print(\"‚úÖ SOLUTION: System will work on CPU (slower but functional)\")\n",
    "print(\"   The system automatically detects and adapts to CPU-only mode\")\n",
    "print(\"   Expected performance: ~2-3x slower than GPU mode\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: Import errors during execution\")\n",
    "print(\"‚úÖ SOLUTION: Ensure virtual environment is activated:\")\n",
    "print(\"   Windows: .\\\\venv\\\\Scripts\\\\activate\")\n",
    "print(\"   Linux/Mac: source venv/bin/activate\")\n",
    "print()\n",
    "\n",
    "print(\"‚ùå PROBLEM: Memory errors during processing\")\n",
    "print(\"‚úÖ SOLUTION: Use quick mode or reduce batch sizes:\")\n",
    "print(\"   Command: python COMPLETE_ENHANCED_RUNNER.py --quick\")\n",
    "print(\"   Or modify batch sizes in the code (documented in scripts)\")\n",
    "print()\n",
    "\n",
    "print(\"üí° GENERAL DEBUGGING TIPS:\")\n",
    "print(\"   1. Check logs in the 'logs/' directory\")\n",
    "print(\"   2. Verify all services are running before starting\")\n",
    "print(\"   3. Use --quick mode for initial testing\")\n",
    "print(\"   4. Monitor system resources (RAM, CPU, disk space)\")\n",
    "print(\"   5. Ensure data_large/ directory has documents to process\")\n",
    "\n",
    "print(\"\\nüìû GETTING HELP:\")\n",
    "print(\"   ‚Ä¢ Check system_config.json for configuration details\")\n",
    "print(\"   ‚Ä¢ Review complete_system_report.json after running\")\n",
    "print(\"   ‚Ä¢ Use the verification cells above to diagnose issues\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
